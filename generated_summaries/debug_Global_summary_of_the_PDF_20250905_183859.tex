
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage[a4paper,margin=25mm]{geometry}
\usepackage{xcolor}
\definecolor{primary}{HTML}{0B6FA4}
\definecolor{secondary}{HTML}{3A6B8A}
\definecolor{accent}{HTML}{6E7B8C}
\definecolor{lightgray}{HTML}{F3F5F7}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\usepackage{sectsty}
\allsectionsfont{\color{primary}\sffamily}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\sffamily\color{primary}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\sffamily\color{secondary}}{\thesubsection}{0.8em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\sffamily\color{accent}}{\thesubsubsection}{0.6em}{}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.6pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\headrule}{\color{lightgray}\hrule height \headrulewidth \vspace{-\headrulewidth}}
\fancyhead[L]{\sffamily\small Course Notes Summary}
\fancyhead[C]{\sffamily\small Focus: Global summary of the PDF}
\fancyhead[R]{\sffamily\small Source: BayesTheorem.pdf}
\fancyfoot[L]{\sffamily\small Generated: 2025-09-05 18:38:59}
\fancyfoot[C]{}
\fancyfoot[R]{\sffamily\small \thepage}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=primary, urlcolor=secondary, citecolor=primary}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}
\raggedbottom
\usepackage{titling}
\pretitle{\begin{center}\vspace*{1cm}\sffamily}
\posttitle{\par\end{center}\vskip 0.2em}
\preauthor{}\postauthor{}
\predate{}\postdate{}
% Helper macros used by model output
\newcommand{\Prob}{\mathrm{P}}
\newcommand{\given}{\,\mid\,}

\begin{document}


\begin{titlepage}
  \thispagestyle{empty}
  \begin{center}
    \vspace*{1cm}
    \begin{center}
    \includegraphics[width=0.3\textwidth]{16.png}
    \end{center}
    \vspace{1em}
    {\sffamily\Huge\bfseries\color{primary} Course Notes Summary \par}
    \vspace{0.8em}
    {\sffamily\Large\color{secondary} Focus Area: Global summary of the PDF \par}
    \vspace{1.5em}
    \begin{tcolorbox}[enhanced,width=0.75\textwidth,colback=lightgray,colframe=primary,boxrule=0.8pt,sharp corners,left=6pt,right=6pt,top=6pt,bottom=6pt]\vspace{0.5em}
      \sffamily
      \begin{tabular}{@{}p{0.28\textwidth} p{0.62\textwidth}@{}}
        \textbf{Source: } & BayesTheorem.pdf \\
        \textbf{Generated:} & 2025-09-05 18:38:59 \\
        \textbf{Summary Type:} & Comprehensive \\
      \end{tabular}
    \end{tcolorbox}

    \vspace{1.5em}
    {\small\sffamily Prepared for quick revision and reference\par}
    \vfill

    {\small\sffamily \color{accent} Use this sheet as a step-by-step guide when solving problems.}
    \vspace{1.8cm}
  \end{center}
\end{titlepage}

\section{Core concepts}

\subsection{Definitions}
\begin{itemize}
\item Prior probability: $P(A)$. The probability of hypothesis $A$ before new data.
\item Likelihood: $P(B\mid A)$. The probability of observed data $B$ assuming hypothesis $A$.
\item Evidence (marginal likelihood): $P(B)$. The overall probability of the observed data.
\item Posterior probability: $P(A\mid B)$. The updated probability of $A$ given observation $B$.
\item Bayes' theorem: a rule to update priors using new evidence.
\end{itemize}

\subsection{Bayes' theorem (basic form)}
\begin{itemize}
\item Equation:
$$
P(A\mid B)=\frac{P(B\mid A)\,P(A)}{P(B)}.
$$
\item Where, for a two-part partition,
$$
P(B)=P(B\mid A)\,P(A)+P(B\mid A^{c})\,P(A^{c}).
$$
\end{itemize}

\subsection{Extended Bayes (multiple hypotheses)}
\begin{itemize}
\item Equation for hypotheses $A,B,C$:
$$
P(A\mid D)=\frac{P(D\mid A)\,P(A)}{P(D\mid A)\,P(A)+P(D\mid B)\,P(B)+P(D\mid C)\,P(C)}.
$$
\item Use when a sample can come from more than two exclusive sources.
\end{itemize}

\section{Illustrative examples}

\subsection{Orange County cigar example (two-group Bayes update)}
\begin{itemize}
\item Given:
\begin{itemize}
\item $P(M)=0.51$ (prior male)
\item $P(F)=0.49$ (prior female)
\item $P(\text{Cigar}\mid M)=0.095$
\item $P(\text{Cigar}\mid F)=0.017$
\end{itemize}
\item Posterior required: $P(M\mid \text{Cigar})$.
\item Formula:
$$
P(M\mid \text{Cigar})=\frac{P(\text{Cigar}\mid M)\,P(M)}{P(\text{Cigar}\mid M)\,P(M)+P(\text{Cigar}\mid F)\,P(F)}.
$$
\item Substitute:
$$
P(M\mid \text{Cigar})=\frac{0.095\times 0.51}{0.095\times 0.51+0.017\times 0.49}.
$$
\item Numeric result:
$$
P(M\mid \text{Cigar})\approx 0.853.
$$
\item Interpretation: Learning the subject smokes a cigar increases the probability they are male.
\end{itemize}

\subsection{ELT manufacturer example (three hypotheses)}
\begin{itemize}
\item Given:
\begin{itemize}
\item $P(A)=0.80,\; P(B)=0.15,\; P(C)=0.05$
\item $P(D\mid A)=0.04,\; P(D\mid B)=0.06,\; P(D\mid C)=0.09$
\end{itemize}
\item Posterior required: $P(A\mid D)$.
\item Formula:
$$
P(A\mid D)=\frac{P(D\mid A)\,P(A)}{P(D\mid A)\,P(A)+P(D\mid B)\,P(B)+P(D\mid C)\,P(C)}.
$$
\item Substitute and compute:
\begin{align*}
\text{numerator} &= 0.80\times 0.04 = 0.032,\\
\text{denominator} &= 0.032 + 0.15\times 0.06 + 0.05\times 0.09 \\
&= 0.032 + 0.009 + 0.0045 = 0.0455.
\end{align*}
\item Numeric result:
$$
P(A\mid D)=\frac{0.032}{0.0455}\approx 0.703.
$$
\item Interpretation: Even with defects, most defective ELTs are still likely from the dominant manufacturer $A$.
\end{itemize}

\section{Intuitive (frequency/table) method}
\begin{itemize}
\item Principle: Assume a convenient total population $N$. Convert probabilities to counts. Compute posteriors by frequency ratios.
\end{itemize}

\subsection{Steps}
\begin{enumerate}
\item Choose $N$ (e.g., $100{,}000$).
\item Compute counts for each hypothesis: $N\times P(\text{hypothesis})$.
\item Compute counts for data within each hypothesis: $N\times P(\text{hypothesis})\times P(\text{data}\mid\text{hypothesis})$.
\item Posterior $P(\text{hypothesis}\mid\text{data})=\dfrac{\text{count(hypothesis and data)}}{\text{total count(data)}}$.
\end{enumerate}

\subsection{Example (Orange County)}
\begin{itemize}
\item $N=100{,}000$ $\Rightarrow$ males $=51{,}000$.
\item Male cigar-smokers $=51{,}000\times 0.095=4{,}845$.
\item Female cigar-smokers $=49{,}000\times 0.017=833$.
\item Total cigar-smokers $=4{,}845+833=5{,}678$.
\item
$$
P(M\mid \text{Cigar})=\frac{4{,}845}{5{,}678}\approx 0.853.
$$
\end{itemize}

\section{Important relationships and connections}
\begin{itemize}
\item Posterior $\propto$ Likelihood $\times$ Prior:
$$
P(A\mid B)\propto P(B\mid A)\,P(A).
$$
\item Evidence normalizes posteriors across all competing hypotheses.
\item Strong likelihood differences can overwhelm moderate priors.
\item Dominant priors (very large differences in $P(A)$) can reduce the effect of likelihood unless likelihoods differ greatly.
\item The table/frequency method reduces calculation errors and aids intuition.
\end{itemize}

\section{Common pitfalls and practical notes}
\begin{itemize}
\item Misplacing conditional probabilities (confusing $P(A\mid B)$ with $P(B\mid A)$).
\item Forgetting to include all partitions in $P(B)$ when computing evidence.
\item Rounding early leads to inaccurate posteriors.
\item Assuming independence incorrectly when events are not independent.
\item Choice of prior matters; sensitivity analysis can be useful.
\end{itemize}

\section{Additional items from the PDF (team contributions and references)}
\begin{itemize}
\item Team member contributions (brief):
\begin{itemize}
\item Member A: UNet, CLIP features, robustness analysis.
\item Member B: Prompt-based segmentation, autoencoder, UX.
\end{itemize}
\item Selected references (representative):
\begin{itemize}
\item Parkhi et al. (2012), ``Cats and dogs,'' CVPR.
\item Radford et al. (2021), ``Learning transferable visual models from natural language supervision,'' ICML.
\item Ronneberger et al. (2015), ``U-Net: convolutional networks for biomedical image segmentation,'' MICCAI.
\item Shorten \& Khoshgoftaar (2019), ``Survey on image data augmentation,'' Journal of Big Data.
\item Simard et al. (2003), ``Best practices for convolutional networks.''
\end{itemize}
\end{itemize}

\section{Quick revision checklist}
\begin{itemize}
\item Understand definitions: prior, likelihood, evidence, posterior.
\item Memorize Bayes' theorem:
$$
P(A\mid B)=\frac{P(B\mid A)\,P(A)}{P(B)}.
$$
\item Use the frequency/table method for clarity and error reduction.
\item Check that evidence includes all mutually exclusive partitions.
\item Perform sensitivity checks on priors when conclusions are critical.
\end{itemize}

\end{document}
