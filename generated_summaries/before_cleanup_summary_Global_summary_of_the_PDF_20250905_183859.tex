\section{Core concepts}

\subsection{Definitions}
\begin{itemize}
\item Prior probability: $P(A)$. The probability of hypothesis $A$ before new data.
\item Likelihood: $P(B\mid A)$. The probability of observed data $B$ assuming hypothesis $A$.
\item Evidence (marginal likelihood): $P(B)$. The overall probability of the observed data.
\item Posterior probability: $P(A\mid B)$. The updated probability of $A$ given observation $B$.
\item Bayes' theorem: a rule to update priors using new evidence.
\end{itemize}

\subsection{Bayes' theorem (basic form)}
\begin{itemize}
\item Equation:
$$
P(A\mid B)=\frac{P(B\mid A)\,P(A)}{P(B)}.
$$
\item Where, for a two-part partition,
$$
P(B)=P(B\mid A)\,P(A)+P(B\mid A^{c})\,P(A^{c}).
$$
\end{itemize}

\subsection{Extended Bayes (multiple hypotheses)}
\begin{itemize}
\item Equation for hypotheses $A,B,C$:
$$
P(A\mid D)=\frac{P(D\mid A)\,P(A)}{P(D\mid A)\,P(A)+P(D\mid B)\,P(B)+P(D\mid C)\,P(C)}.
$$
\item Use when a sample can come from more than two exclusive sources.
\end{itemize}

\section{Illustrative examples}

\subsection{Orange County cigar example (two-group Bayes update)}
\begin{itemize}
\item Given:
\begin{itemize}
\item $P(M)=0.51$ (prior male)
\item $P(F)=0.49$ (prior female)
\item $P(\text{Cigar}\mid M)=0.095$
\item $P(\text{Cigar}\mid F)=0.017$
\end{itemize}
\item Posterior required: $P(M\mid \text{Cigar})$.
\item Formula:
$$
P(M\mid \text{Cigar})=\frac{P(\text{Cigar}\mid M)\,P(M)}{P(\text{Cigar}\mid M)\,P(M)+P(\text{Cigar}\mid F)\,P(F)}.
$$
\item Substitute:
$$
P(M\mid \text{Cigar})=\frac{0.095\times 0.51}{0.095\times 0.51+0.017\times 0.49}.
$$
\item Numeric result:
$$
P(M\mid \text{Cigar})\approx 0.853.
$$
\item Interpretation: Learning the subject smokes a cigar increases the probability they are male.
\end{itemize}

\subsection{ELT manufacturer example (three hypotheses)}
\begin{itemize}
\item Given:
\begin{itemize}
\item $P(A)=0.80,\; P(B)=0.15,\; P(C)=0.05$
\item $P(D\mid A)=0.04,\; P(D\mid B)=0.06,\; P(D\mid C)=0.09$
\end{itemize}
\item Posterior required: $P(A\mid D)$.
\item Formula:
$$
P(A\mid D)=\frac{P(D\mid A)\,P(A)}{P(D\mid A)\,P(A)+P(D\mid B)\,P(B)+P(D\mid C)\,P(C)}.
$$
\item Substitute and compute:
\begin{align*}
\text{numerator} &= 0.80\times 0.04 = 0.032,\\
\text{denominator} &= 0.032 + 0.15\times 0.06 + 0.05\times 0.09 \\
&= 0.032 + 0.009 + 0.0045 = 0.0455.
\end{align*}
\item Numeric result:
$$
P(A\mid D)=\frac{0.032}{0.0455}\approx 0.703.
$$
\item Interpretation: Even with defects, most defective ELTs are still likely from the dominant manufacturer $A$.
\end{itemize}

\section{Intuitive (frequency/table) method}
\begin{itemize}
\item Principle: Assume a convenient total population $N$. Convert probabilities to counts. Compute posteriors by frequency ratios.
\end{itemize}

\subsection{Steps}
\begin{enumerate}
\item Choose $N$ (e.g., $100{,}000$).
\item Compute counts for each hypothesis: $N\times P(\text{hypothesis})$.
\item Compute counts for data within each hypothesis: $N\times P(\text{hypothesis})\times P(\text{data}\mid\text{hypothesis})$.
\item Posterior $P(\text{hypothesis}\mid\text{data})=\dfrac{\text{count(hypothesis and data)}}{\text{total count(data)}}$.
\end{enumerate}

\subsection{Example (Orange County)}
\begin{itemize}
\item $N=100{,}000$ $\Rightarrow$ males $=51{,}000$.
\item Male cigar-smokers $=51{,}000\times 0.095=4{,}845$.
\item Female cigar-smokers $=49{,}000\times 0.017=833$.
\item Total cigar-smokers $=4{,}845+833=5{,}678$.
\item
$$
P(M\mid \text{Cigar})=\frac{4{,}845}{5{,}678}\approx 0.853.
$$
\end{itemize}

\section{Important relationships and connections}
\begin{itemize}
\item Posterior $\propto$ Likelihood $\times$ Prior:
$$
P(A\mid B)\propto P(B\mid A)\,P(A).
$$
\item Evidence normalizes posteriors across all competing hypotheses.
\item Strong likelihood differences can overwhelm moderate priors.
\item Dominant priors (very large differences in $P(A)$) can reduce the effect of likelihood unless likelihoods differ greatly.
\item The table/frequency method reduces calculation errors and aids intuition.
\end{itemize}

\section{Common pitfalls and practical notes}
\begin{itemize}
\item Misplacing conditional probabilities (confusing $P(A\mid B)$ with $P(B\mid A)$).
\item Forgetting to include all partitions in $P(B)$ when computing evidence.
\item Rounding early leads to inaccurate posteriors.
\item Assuming independence incorrectly when events are not independent.
\item Choice of prior matters; sensitivity analysis can be useful.
\end{itemize}

\section{Additional items from the PDF (team contributions and references)}
\begin{itemize}
\item Team member contributions (brief):
\begin{itemize}
\item Member A: UNet, CLIP features, robustness analysis.
\item Member B: Prompt-based segmentation, autoencoder, UX.
\end{itemize}
\item Selected references (representative):
\begin{itemize}
\item Parkhi et al. (2012), ``Cats and dogs,'' CVPR.
\item Radford et al. (2021), ``Learning transferable visual models from natural language supervision,'' ICML.
\item Ronneberger et al. (2015), ``U-Net: convolutional networks for biomedical image segmentation,'' MICCAI.
\item Shorten \& Khoshgoftaar (2019), ``Survey on image data augmentation,'' Journal of Big Data.
\item Simard et al. (2003), ``Best practices for convolutional networks.''
\end{itemize}
\end{itemize}

\section{Quick revision checklist}
\begin{itemize}
\item Understand definitions: prior, likelihood, evidence, posterior.
\item Memorize Bayes' theorem:
$$
P(A\mid B)=\frac{P(B\mid A)\,P(A)}{P(B)}.
$$
\item Use the frequency/table method for clarity and error reduction.
\item Check that evidence includes all mutually exclusive partitions.
\item Perform sensitivity checks on priors when conclusions are critical.
\end{itemize}