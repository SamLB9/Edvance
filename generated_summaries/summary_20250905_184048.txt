1. Focus area: e;ibeofge

1.1 Definition and scope
- Interpreted here as the extension and intuitive application of Bayes' theorem to multiple competing hypotheses.
- Applies when an initial event can arise from one of several mutually exclusive, exhaustive causes.
- Useful for diagnostic reasoning, manufacturing-fault attribution, and any problem with alternative source hypotheses.

2. Key definitions and formulas

2.1 Basic definitions
- Conditional probability: P(A|B) is the probability of A given B.
- Prior probability: P(A) is the probability of hypothesis A before observing evidence.
- Likelihood: P(Z|A) is the probability of observing evidence Z assuming A is true.
- Posterior probability: P(A|Z) is the updated probability of A after observing Z.
- Partition: A set {A, B, C, ...} is a partition if events are mutually exclusive and cover the sample space.

2.2 Core formulas
- Bayes' theorem (two events): P(A|Z) = P(A)P(Z|A) / P(Z)
- Law of total probability: P(Z) = sum over i of P(H_i)P(Z|H_i)
- Extension to four hypotheses A, B, C, D:
  P(A|Z) = P(A)P(Z|A) / [P(A)P(Z|A) + P(B)P(Z|B) + P(C)P(Z|C) + P(D)P(Z|D)]

3. Procedure to compute posterior for one hypothesis

3.1 Steps (ordered)
1 - Identify the hypothesis set {A, B, C, ...} that partitions outcomes.
2 - Specify priors P(A), P(B), etc. Ensure they sum to 1.
3 - Specify likelihoods P(Z|A), P(Z|B), etc.
4 - Compute denominator P(Z) by summing P(H_i)P(Z|H_i).
5 - Compute posterior P(A|Z) = P(A)P(Z|A) / P(Z).

3.2 Intuitive frequency-table method
- Pick a convenient total N (e.g., 10,000).
- For each hypothesis H:
  - Count = N * P(H).
  - Evidence-count = Count * P(Z|H).
- Sum evidence-counts to get total occurrences of Z.
- Posterior for A = evidence-count(A) / total evidence-counts.

4. Example (ELT manufacturing, concrete numbers)
- Given:
  - P(A) = 0.80, P(Z|A)=0.04
  - P(B) = 0.15, P(Z|B)=0.06
  - P(C) = 0.05, P(Z|C)=0.09
- Use N = 10,000:
  - A: 8,000 units; defective = 8,000 * 0.04 = 320
  - B: 1,500 units; defective = 1,500 * 0.06 = 90
  - C: 500 units; defective = 500 * 0.09 = 45
  - Total defective = 320 + 90 + 45 = 455
- Posterior:
  - P(A|defective) = 320 / 455 ≈ 0.703

5. Important relationships and connections
- Posterior depends on both prior and likelihood. Strong likelihood can overcome a small prior.
- Law of total probability is required to normalize posterior probabilities.
- Frequentist (count-based) intuition and Bayesian formula give identical results when applied consistently.
- Diagnostic testing connection:
  - Sensitivity = P(test positive | disease).
  - Specificity = P(test negative | no disease).
  - Bayes' theorem gives P(disease | test result), which depends on disease prevalence (the prior).
- Adding more hypotheses extends denominator by additional terms P(H_i)P(Z|H_i).

6. Common pitfalls and practical tips
- Ensure hypotheses are mutually exclusive and exhaustive before applying the formula.
- Do not mix conditional directions: keep P(Z|H) for likelihoods and P(H) for priors.
- Small priors and imperfect tests can produce counterintuitive posteriors (low posterior even with positive test).
- Use a frequency-table method to check arithmetic and to build intuition.
- Include a small epsilon only when needed for numerical stability in computations of continuous densities.

7. Quick reference (compact)
- Bayes' theorem: P(A|Z) = P(A)P(Z|A) / P(Z)
- Total probability: P(Z) = sum_i P(H_i)P(Z|H_i)
- Four-hypothesis posterior:
  P(A|Z) = P(A)P(Z|A) / [P(A)P(Z|A) + P(B)P(Z|B) + P(C)P(Z|C) + P(D)P(Z|D)]

8. Short summary
- Bayes' theorem updates priors using likelihoods to produce posteriors.
- For multiple causes, normalize by summing all prior×likelihood terms.
- Frequency tables provide a simple, robust check and clear intuition.