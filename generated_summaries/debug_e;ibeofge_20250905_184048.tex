
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage[a4paper,margin=25mm]{geometry}
\usepackage{xcolor}
\definecolor{primary}{HTML}{0B6FA4}
\definecolor{secondary}{HTML}{3A6B8A}
\definecolor{accent}{HTML}{6E7B8C}
\definecolor{lightgray}{HTML}{F3F5F7}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\usepackage{sectsty}
\allsectionsfont{\color{primary}\sffamily}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\sffamily\color{primary}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\sffamily\color{secondary}}{\thesubsection}{0.8em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\sffamily\color{accent}}{\thesubsubsection}{0.6em}{}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.6pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\headrule}{\color{lightgray}\hrule height \headrulewidth \vspace{-\headrulewidth}}
\fancyhead[L]{\sffamily\small Course Notes Summary}
\fancyhead[C]{\sffamily\small Focus: e;ibeofge}
\fancyhead[R]{\sffamily\small Source: BayesTheorem.pdf}
\fancyfoot[L]{\sffamily\small Generated: 2025-09-05 18:40:48}
\fancyfoot[C]{}
\fancyfoot[R]{\sffamily\small \thepage}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=primary, urlcolor=secondary, citecolor=primary}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}
\raggedbottom
\usepackage{titling}
\pretitle{\begin{center}\vspace*{1cm}\sffamily}
\posttitle{\par\end{center}\vskip 0.2em}
\preauthor{}\postauthor{}
\predate{}\postdate{}
% Helper macros used by model output
\newcommand{\Prob}{\mathrm{P}}
\newcommand{\given}{\,\mid\,}

\begin{document}


\begin{titlepage}
  \thispagestyle{empty}
  \begin{center}
    \vspace*{1cm}
    \begin{center}
    \includegraphics[width=0.3\textwidth]{16.png}
    \end{center}
    \vspace{1em}
    {\sffamily\Huge\bfseries\color{primary} Course Notes Summary \par}
    \vspace{0.8em}
    {\sffamily\Large\color{secondary} Focus Area: e;ibeofge \par}
    \vspace{1.5em}
    \begin{tcolorbox}[enhanced,width=0.75\textwidth,colback=lightgray,colframe=primary,boxrule=0.8pt,sharp corners,left=6pt,right=6pt,top=6pt,bottom=6pt]\vspace{0.5em}
      \sffamily
      \begin{tabular}{@{}p{0.28\textwidth} p{0.62\textwidth}@{}}
        \textbf{Source: } & BayesTheorem.pdf \\
        \textbf{Generated:} & 2025-09-05 18:40:48 \\
        \textbf{Summary Type:} & Comprehensive \\
      \end{tabular}
    \end{tcolorbox}

    \vspace{1.5em}
    {\small\sffamily Prepared for quick revision and reference\par}
    \vfill

    {\small\sffamily \color{accent} Use this sheet as a step-by-step guide when solving problems.}
    \vspace{1.8cm}
  \end{center}
\end{titlepage}

\section{Focus area: e;ibeofge}

\subsection{Definition and scope}
\begin{itemize}
\item Interpreted here as the extension and intuitive application of Bayes' theorem to multiple competing hypotheses.
\item Applies when an initial event can arise from one of several mutually exclusive, exhaustive causes.
\item Useful for diagnostic reasoning, manufacturing-fault attribution, and any problem with alternative source hypotheses.
\end{itemize}

\section{Key definitions and formulas}

\subsection{Basic definitions}
\begin{itemize}
\item Conditional probability: $P(A\mid B)$ is the probability of $A$ given $B$.
\item Prior probability: $P(A)$ is the probability of hypothesis $A$ before observing evidence.
\item Likelihood: $P(Z\mid A)$ is the probability of observing evidence $Z$ assuming $A$ is true.
\item Posterior probability: $P(A\mid Z)$ is the updated probability of $A$ after observing $Z$.
\item Partition: a set $\{A, B, C, \ldots\}$ is a partition if events are mutually exclusive and cover the sample space.
\end{itemize}

\subsection{Core formulas}
\begin{itemize}
\item Bayes' theorem (two events):
$$
P(A\mid Z)=\frac{P(A)P(Z\mid A)}{P(Z)}.
$$
\item Law of total probability:
$$
P(Z)=\sum_i P(H_i)P(Z\mid H_i).
$$
\item Extension to four hypotheses $A,B,C,D$:
$$
P(A\mid Z)=\frac{P(A)P(Z\mid A)}{P(A)P(Z\mid A)+P(B)P(Z\mid B)+P(C)P(Z\mid C)+P(D)P(Z\mid D)}.
$$
\end{itemize}

\section{Procedure to compute posterior for one hypothesis}

\subsection{Steps (ordered)}
\begin{enumerate}
\item Identify the hypothesis set $\{A, B, C, \ldots\}$ that partitions outcomes.
\item Specify priors $P(A),P(B),\ldots$. Ensure they sum to 1: $\sum_i P(H_i)=1$.
\item Specify likelihoods $P(Z\mid A),P(Z\mid B),\ldots$.
\item Compute denominator $P(Z)$ by summing $P(H_i)P(Z\mid H_i)$.
\item Compute posterior
$$
P(A\mid Z)=\frac{P(A)P(Z\mid A)}{P(Z)}.
$$
\end{enumerate}

\subsection{Intuitive frequency-table method}
\begin{itemize}
\item Pick a convenient total $N$ (e.g., $10{,}000$).
\item For each hypothesis $H$:
  \begin{itemize}
  \item Count $=N\cdot P(H)$.
  \item Evidence-count $=\text{Count}\cdot P(Z\mid H)$.
  \end{itemize}
\item Sum evidence-counts to get total occurrences of $Z$.
\item Posterior for $A$:
$$
P(A\mid Z)=\frac{\text{evidence-count}(A)}{\sum_H \text{evidence-count}(H)}.
$$
\end{itemize}

\section{Example (ELT manufacturing, concrete numbers)}
\begin{itemize}
\item Given:
  \begin{itemize}
  \item $P(A)=0.80,\; P(Z\mid A)=0.04$
  \item $P(B)=0.15,\; P(Z\mid B)=0.06$
  \item $P(C)=0.05,\; P(Z\mid C)=0.09$
  \end{itemize}
\item Use $N=10{,}000$:
  \begin{itemize}
  \item A: $8{,}000$ units; defective $=8{,}000\times 0.04=320$.
  \item B: $1{,}500$ units; defective $=1{,}500\times 0.06=90$.
  \item C: $500$ units; defective $=500\times 0.09=45$.
  \item Total defective $=320+90+45=455$.
  \end{itemize}
\item Posterior:
$$
P(A\mid \text{defective})=\frac{320}{455}\approx 0.703.
$$
\end{itemize}

\section{Important relationships and connections}
\begin{itemize}
\item Posterior depends on both prior and likelihood. A strong likelihood can overcome a small prior.
\item The law of total probability is required to normalize posterior probabilities.
\item Frequentist (count-based) intuition and the Bayesian formula give identical results when applied consistently.
\item Diagnostic testing connection:
  \begin{itemize}
  \item Sensitivity $=P(\text{test positive}\mid \text{disease})$.
  \item Specificity $=P(\text{test negative}\mid \text{no disease})$.
  \item Bayes' theorem gives $P(\text{disease}\mid \text{test result})$, which depends on disease prevalence (the prior).
  \end{itemize}
\item Adding more hypotheses extends the denominator by additional terms $P(H_i)P(Z\mid H_i)$.
\end{itemize}

\section{Common pitfalls and practical tips}
\begin{itemize}
\item Ensure hypotheses are mutually exclusive and exhaustive before applying the formula.
\item Do not mix conditional directions: keep $P(Z\mid H)$ for likelihoods and $P(H)$ for priors.
\item Small priors and imperfect tests can produce counterintuitive posteriors (low posterior even with a positive test).
\item Use a frequency-table method to check arithmetic and to build intuition.
\item Include a small $\varepsilon$ only when needed for numerical stability in computations of continuous densities.
\end{itemize}

\section{Quick reference (compact)}
\begin{itemize}
\item Bayes' theorem:
$$
P(A\mid Z)=\frac{P(A)P(Z\mid A)}{P(Z)}
$$
\item Total probability:
$$
P(Z)=\sum_i P(H_i)P(Z\mid H_i)
$$
\item Four-hypothesis posterior:
$$
P(A\mid Z)=\frac{P(A)P(Z\mid A)}{P(A)P(Z\mid A)+P(B)P(Z\mid B)+P(C)P(Z\mid C)+P(D)P(Z\mid D)}
$$
\end{itemize}

\section{Short summary}
\begin{itemize}
\item Bayes' theorem updates priors using likelihoods to produce posteriors.
\item For multiple causes, normalize by summing all prior$\times$likelihood terms.
\item Frequency tables provide a simple, robust check and clear intuition.
\end{itemize}

\end{document}
