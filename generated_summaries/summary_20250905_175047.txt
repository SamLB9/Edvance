FOCUS AREA: Brief summary of key formulas

Definitions
- Conditional probability: P(B|A) is the probability of B given A has occurred.
- Prior probability: Initial probability of a hypothesis before new evidence. (Write as P(A).)
- Posterior probability: Revised probability after observing evidence. (Write as P(A|B).)
- Likelihood: Probability of the evidence under a hypothesis. (Write as P(B|A).)
- Evidence (marginal probability): Total probability of observed evidence. (Write as P(B).)

Core formulas
- Conditional probability: P(B|A) = P(A and B) / P(A)
- Bayes' theorem (two events): P(A|B) = P(B|A) * P(A) / P(B)
- Law of total probability (for evidence): P(B) = sum_i [P(B|A_i) * P(A_i)]
- Bayes' theorem (generalized): P(A_j|B) = P(B|A_j) * P(A_j) / sum_i [P(B|A_i) * P(A_i)]

Step-by-step procedure to apply Bayes' theorem
1 - Identify the hypotheses A_i that partition the sample space.  
2 - Record priors P(A_i) for each hypothesis.  
3 - Obtain likelihoods P(B|A_i) for the observed evidence B.  
4 - Compute the evidence P(B) = sum_i [P(B|A_i) * P(A_i)].  
5 - Compute posteriors P(A_j|B) = P(B|A_j) * P(A_j) / P(B).

Intuitive (frequency) method
- Assume a convenient total N (e.g., 1000 or 10000).  
- For each hypothesis A_i compute count_i = N * P(A_i).  
- For each hypothesis compute evidence_count_i = count_i * P(B|A_i).  
- Total evidence_count = sum_i evidence_count_i.  
- Then P(A_j|B) = evidence_count_j / total evidence_count.  
- This method is algebraically identical to Bayes' theorem. It reduces algebra errors.

Worked examples (concise)
- Example: Orange County cigar smoker
  - Given: P(male) = 0.51, P(female) = 0.49.
  - Likelihoods: P(smokes|male) = 0.095, P(smokes|female) = 0.017.
  - Use Bayes: P(male|smokes) = 0.095*0.51 / [0.095*0.51 + 0.017*0.49]
  - Calculation: numerator = 0.04845. denominator = 0.04845 + 0.00833 = 0.05678.
  - Result: P(male|smokes) ≈ 0.853 (85.3%).

- Example: ELT manufacturers (A, B, C) and defect D
  - Given: P(A)=0.80, P(B)=0.15, P(C)=0.05.
  - Likelihoods: P(D|A)=0.04, P(D|B)=0.06, P(D|C)=0.09.
  - Use Bayes for P(A|D): = 0.80*0.04 / [0.80*0.04 + 0.15*0.06 + 0.05*0.09]
  - Calculation: numerator = 0.032. denominator = 0.032 + 0.009 + 0.0045 = 0.0455.
  - Result: P(A|D) ≈ 0.703 (70.3%).
  - Frequency check (N=10000): A=8000 defects=320; B=1500 defects=90; C=500 defects=45; P(A|D)=320/455≈0.703.

Important relationships and connections
- Bayes inverts conditional probabilities: P(A|B) relates to P(B|A).  
- The denominator in Bayes ensures normalization over all hypotheses.  
- Bayes relies on exhaustive, mutually exclusive hypotheses A_i for correct denominator.  
- Repeated updating: Posteriors can serve as new priors when new evidence arrives.  
- The frequency interpretation often improves intuition and reduces substitution errors.

Common pitfalls and quick checks
- Do not confuse P(A|B) with P(B|A).  
- Ensure hypotheses A_i sum to 1 (exhaustive).  
- Include all relevant A_i in the law of total probability.  
- Check units by using the frequency method to validate algebraic results.  

Concise reference list of formulas (plain text)
- P(B|A) = P(A and B) / P(A)  
- P(A and B) = P(B|A) * P(A) = P(A|B) * P(B)  
- P(A|B) = P(B|A) * P(A) / P(B)  
- P(B) = sum_i [P(B|A_i) * P(A_i)]  
- P(A_j|B) = P(B|A_j) * P(A_j) / sum_i [P(B|A_i) * P(A_i)]