1. Key definitions
- Prior probability: initial probability before new information.  
- Posterior probability: probability revised after new information.  
- Conditional probability: probability of B given A. Denoted P(B|A).  
- Joint probability: probability that A and B both occur. Denoted P(A and B) or P(A ∩ B).  
- Likelihood: P(data | hypothesis). In Bayes context, P(B|A) often called the likelihood of B under A.

2. Core formulas
2.1 Conditional probability
- P(B|A) = P(A and B) / P(A)
- Multiplication rule: P(A and B) = P(A) P(B|A) = P(B) P(A|B)

2.2 Law of total probability
- Two-event form: P(B) = P(A)P(B|A) + P(A^c)P(B|A^c)
- General form: If events E1, E2, ..., En are disjoint and exhaustive, then
  P(B) = sum over i of [ P(Ei) P(B|Ei) ]

2.3 Bayes' theorem (basic)
- Two events: P(A|B) = P(B|A) P(A) / P(B)
- Using total probability for P(B): P(A|B) = P(B|A) P(A) / [ P(B|A) P(A) + P(B|A^c) P(A^c) ]

2.4 Bayes' theorem (generalized)
- For partition E1,...,En:
  P(Ek|B) = P(Ek) P(B|Ek) / [ sum over i of P(Ei) P(B|Ei) ]

2.5 Odds and likelihood ratio (useful form)
- Posterior odds = Prior odds × Likelihood ratio
- [P(A|B) / P(A^c|B)] = [P(A) / P(A^c)] × [P(B|A) / P(B|A^c)]

3. Practical steps to apply Bayes
1 - Identify hypotheses (E1,...,En).  
1 - Determine priors P(Ei).  
1 - Determine likelihoods P(data | Ei).  
1 - Compute total probability of data: P(data) = sum_i P(Ei) P(data|Ei).  
1 - Compute posteriors: P(Ek|data) = P(Ek) P(data|Ek) / P(data).

4. Intuitive (table) method
- Assume a convenient total population (e.g., 100,000 or 10,000).  
- Compute expected counts for each hypothesis: count(Ei) = total × P(Ei).  
- For each hypothesis, compute counts of data: count(Ei and data) = count(Ei) × P(data|Ei).  
- Sum counts of data over all hypotheses to get total data count.  
- Posterior for Ek = count(Ek and data) / total data count.  
- This avoids algebraic mistakes and clarifies numerators and denominators.

5. Examples
- Example 1 (cigar smoking, Orange County)
  - Given: P(male)=0.51. P(female)=0.49. P(cigar|male)=0.095. P(cigar|female)=0.017.
  - Use Bayes: P(male|cigar) = 0.095·0.51 / [0.095·0.51 + 0.017·0.49]
  - Compute: numerator = 0.04845. denominator = 0.04845 + 0.00833 = 0.05678.
  - Result: P(male|cigar) ≈ 0.853.

- Example 2 (ELT manufacturers and defects)
  - Given: P(A)=0.80, P(B)=0.15, P(C)=0.05. P(def|A)=0.04, P(def|B)=0.06, P(def|C)=0.09.
  - Bayes: P(A|def) = 0.80·0.04 / [0.80·0.04 + 0.15·0.06 + 0.05·0.09]
  - Compute: numerator = 0.032. denominator = 0.032 + 0.009 + 0.0045 = 0.0455.
  - Result: P(A|def) ≈ 0.703.
  - Intuitive table method (assume 10,000 ELTs):
    - A total = 8,000; defects from A = 8,000·0.04 = 320.
    - B total = 1,500; defects from B = 1,500·0.06 = 90.
    - C total = 500; defects from C = 500·0.09 = 45.
    - Total defects = 320 + 90 + 45 = 455.
    - Posterior P(A|def) = 320 / 455 ≈ 0.703.

6. Important relationships and notes
- Bayes updates priors into posteriors using new evidence.  
- Total probability supplies the normalizing denominator for Bayes.  
- If A and B are independent then P(B|A) = P(B). Bayes then reduces to P(A|B) = P(A).  
- Use the table method to reduce algebraic errors.  
- Keep units consistent (probabilities vs counts).  
- For multiple pieces of independent evidence, multiply likelihoods: P(data1, data2 | Ei) = P(data1|Ei) P(data2|Ei) if independent.